import streamlit as st
import numpy as np
import time
from environment import GridEnvironment
from dqn_agent import DQNAgent
from visualization import visualize_grid, visualize_training_stats, visualize_q_values

st.set_page_config(page_title="DQN Grid Navigation", layout="wide")

st.title("ğŸ§  Deep Q-Network (DQN): Grid Navigation")
st.markdown("""
Using **Neural Network** instead of Q-table to learn pathfinding
- ğŸ”¥ Experience Replay: Learn from past experiences
- ğŸ¯ Target Network: Stable training
- ğŸš€ Scales to larger state spaces
""")

# Sidebar controls
st.sidebar.header("âš™ï¸ Configuration")

# Grid size
grid_size = st.sidebar.slider("Grid size (n Ã— n)", 3, 15, 5)

# Training parameters
st.sidebar.subheader("Training Parameters")
episodes = st.sidebar.number_input("Episodes", 100, 10000, 2000, 100)
learning_rate = st.sidebar.slider("Learning rate", 0.0001, 0.01, 0.001, 0.0001, format="%.4f")
discount_factor = st.sidebar.slider("Discount factor (Î³)", 0.01, 1.0, 0.95, 0.01)
epsilon = st.sidebar.slider("Epsilon (exploration)", 0.0, 1.0, 1.0, 0.01)
epsilon_decay = st.sidebar.slider("Epsilon decay", 0.9, 0.999, 0.995, 0.001)
batch_size = st.sidebar.slider("Batch size", 16, 128, 32, 16)
hidden_size = st.sidebar.slider("Hidden layer size", 32, 256, 64, 32)

# Initialize session state
if 'trained_dqn' not in st.session_state:
    st.session_state.trained_dqn = False
    st.session_state.agent_dqn = None
    st.session_state.env_dqn = None
    st.session_state.training_history_dqn = None

# Create environment and agent
env = GridEnvironment(grid_size)

# Training section
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ğŸ“ Train DQN Agent")
    
    if st.button("ğŸš€ Start DQN Training", type="primary"):
        # Táº¡o agent má»›i
        agent = DQNAgent(
            state_size=grid_size,
            action_size=4,
            learning_rate=learning_rate,
            discount_factor=discount_factor,
            epsilon=epsilon,
            epsilon_decay=epsilon_decay,
            batch_size=batch_size,
            hidden_size=hidden_size
        )
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Training metrics
        rewards_history = []
        steps_history = []
        epsilon_history = []
        
        # Training loop
        for episode in range(episodes):
            state = env.reset()
            total_reward = 0
            steps = 0
            done = False
            
            while not done and steps < grid_size * grid_size * 2:
                action = agent.choose_action(state)
                next_state, reward, done = env.step(action)
                agent.learn(state, action, reward, next_state, done)
                
                state = next_state
                total_reward += reward
                steps += 1
            
            rewards_history.append(total_reward)
            steps_history.append(steps)
            epsilon_history.append(agent.epsilon)
            
            # Update progress
            if episode % 10 == 0:
                progress_bar.progress((episode + 1) / episodes)
                avg_reward = np.mean(rewards_history[-100:]) if len(rewards_history) >= 100 else np.mean(rewards_history)
                avg_steps = np.mean(steps_history[-100:]) if len(steps_history) >= 100 else np.mean(steps_history)
                status_text.text(f"Episode {episode+1}/{episodes} | Avg Reward: {avg_reward:.2f} | Avg Steps: {avg_steps:.1f} | Îµ: {agent.epsilon:.3f} | Buffer: {len(agent.replay_buffer)}")
        
        progress_bar.progress(1.0)
        status_text.success(f"âœ… Training completed: {episodes} episodes!")
        
        # Save to session state
        st.session_state.trained_dqn = True
        st.session_state.agent_dqn = agent
        st.session_state.env_dqn = env
        st.session_state.training_history_dqn = {
            'rewards': rewards_history,
            'steps': steps_history,
            'epsilon': epsilon_history
        }

with col2:
    st.header("ğŸ“Š Info")
    if st.session_state.trained_dqn:
        st.metric("Status", "âœ… Trained")
        history = st.session_state.training_history_dqn
        st.metric("Avg Reward (last 100 episodes)", 
                  f"{np.mean(history['rewards'][-100:]):.2f}")
        st.metric("Avg Steps (last 100 episodes)", 
                  f"{np.mean(history['steps'][-100:]):.1f}")
        st.metric("Replay Buffer size", 
                  f"{len(st.session_state.agent_dqn.replay_buffer)}")
    else:
        st.metric("Status", "â³ Not trained")

# Training statistics
if st.session_state.trained_dqn:
    st.header("ğŸ“ˆ Training Statistics")
    visualize_training_stats(st.session_state.training_history_dqn)

# Testing section
st.header("ğŸ® Test Agent")

if st.session_state.trained_dqn:
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("â–¶ï¸ Run 1 Episode", type="secondary"):
            agent = st.session_state.agent_dqn
            env = st.session_state.env_dqn
            
            state = env.reset()
            path = [state]
            total_reward = 0
            steps = 0
            done = False
            
            st.write("### ğŸ—ºï¸ Path:")
            step_container = st.empty()
            grid_container = st.empty()
            
            # Greedy mode
            old_epsilon = agent.epsilon
            agent.epsilon = 0
            
            while not done and steps < grid_size * grid_size * 2:
                action = agent.choose_action(state)
                next_state, reward, done = env.step(action)
                
                state = next_state
                path.append(state)
                total_reward += reward
                steps += 1
                
                # Visualize
                with grid_container:
                    visualize_grid(env.grid_size, path, state)
                
                with step_container:
                    action_names = ['â†‘ Up', 'â†“ Down', 'â† Left', 'â†’ Right']
                    st.text(f"Step {steps}: {action_names[action]} | Position: {state} | Reward: {reward:.1f}")
                
                time.sleep(0.3)
            
            agent.epsilon = old_epsilon  # Restore epsilon
            
            st.write(f"**Result:** {'ğŸ‰ Success!' if done and reward > 0 else 'âŒ Failed'}")
            st.write(f"**Total Reward:** {total_reward:.2f}")
            st.write(f"**Steps:** {steps}")
            st.write(f"**Path:** {' â†’ '.join([str(p) for p in path])}")
    
    with col2:
        if st.checkbox("Show Q-values (DQN)"):
            st.write("### ğŸ§  Q-values from Neural Network")
            visualize_q_values(st.session_state.agent_dqn, grid_size)

else:
    st.warning("âš ï¸ Please train the DQN agent first!")

# Comparison section
st.header("ğŸ”„ So sÃ¡nh Q-Learning vs DQN")

with st.expander("ğŸ“š Q-Learning (Q-table) vs DQN (Neural Network)"):
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Q-Learning (Q-table)")
        st.markdown("""
        **Æ¯u Ä‘iá»ƒm:**
        - âœ… ÄÆ¡n giáº£n, dá»… hiá»ƒu
        - âœ… Guaranteed convergence (vá»›i Ä‘iá»u kiá»‡n phÃ¹ há»£p)
        - âœ… KhÃ´ng cáº§n training neural network
        - âœ… Fast inference
        
        **NhÆ°á»£c Ä‘iá»ƒm:**
        - âŒ KhÃ´ng scale vá»›i state space lá»›n
        - âŒ Chá»‰ lÃ m viá»‡c vá»›i discrete states
        - âŒ Cáº§n visit má»—i state nhiá»u láº§n
        - âŒ KhÃ´ng cÃ³ generalization
        
        **Khi nÃ o dÃ¹ng:**
        - State space nhá» (< 10,000 states)
        - Discrete states
        - Cáº§n solution Ä‘Æ¡n giáº£n
        """)
    
    with col2:
        st.subheader("DQN (Neural Network)")
        st.markdown("""
        **Æ¯u Ä‘iá»ƒm:**
        - âœ… Scale tá»‘t vá»›i state space lá»›n
        - âœ… Generalization: há»c tá»« similar states
        - âœ… CÃ³ thá»ƒ lÃ m vá»›i continuous states
        - âœ… Experience Replay: há»c hiá»‡u quáº£ hÆ¡n
        
        **NhÆ°á»£c Ä‘iá»ƒm:**
        - âŒ Phá»©c táº¡p hÆ¡n
        - âŒ Cáº§n tune nhiá»u hyperparameters
        - âŒ Training cháº­m hÆ¡n
        - âŒ KhÃ´ng guaranteed convergence
        
        **Khi nÃ o dÃ¹ng:**
        - State space lá»›n (> 10,000 states)
        - Continuous states
        - Cáº§n generalization
        - CÃ³ GPU Ä‘á»ƒ training
        """)

with st.expander("ğŸ”¬ CÃ¡c ká»¹ thuáº­t trong DQN"):
    st.markdown("""
    ### 1. Experience Replay
    - LÆ°u trá»¯ experiences (s, a, r, s') trong buffer
    - Sample ngáº«u nhiÃªn batch Ä‘á»ƒ training
    - **Lá»£i Ã­ch**: PhÃ¡ vá»¡ correlation giá»¯a sequential samples
    
    ### 2. Target Network
    - DÃ¹ng 2 networks: Policy Network vÃ  Target Network
    - Target Network update cháº­m hÆ¡n (má»—i N steps)
    - **Lá»£i Ã­ch**: Training stable hÆ¡n, trÃ¡nh oscillation
    
    ### 3. Epsilon-Greedy vá»›i Decay
    - Báº¯t Ä‘áº§u vá»›i epsilon cao (exploration)
    - Giáº£m dáº§n epsilon theo thá»i gian
    - **Lá»£i Ã­ch**: Balance exploration vÃ  exploitation
    
    ### 4. Gradient Clipping
    - Giá»›i háº¡n gradient magnitude
    - **Lá»£i Ã­ch**: TrÃ¡nh exploding gradients
    
    ### Network Architecture
    ```
    Input (2): x, y coordinates (normalized)
    â†“
    Dense(64) + ReLU
    â†“
    Dense(64) + ReLU
    â†“
    Output(4): Q-values cho 4 actions
    ```
    """)